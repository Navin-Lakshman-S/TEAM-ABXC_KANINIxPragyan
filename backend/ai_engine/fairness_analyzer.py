"""
Fairness & Bias Analyzer
-------------------------
Evaluates model fairness across demographic groups (gender, age).
Computes demographic parity, equalized odds, disparate impact ratio,
and per-group performance metrics.
"""

import os
import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score, classification_report

BASE = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE, "..", "models")
DATA_DIR = os.path.join(BASE, "..", "data")


def _load_artifacts():
    from backend.ai_engine.train_model import prepare_features
    model = joblib.load(os.path.join(MODEL_DIR, "triage_model.joblib"))
    le = joblib.load(os.path.join(MODEL_DIR, "label_encoder.joblib"))
    return model, le, prepare_features


def analyze_fairness() -> dict:
    """
    Run a comprehensive fairness audit on the synthetic dataset
    using the trained model. Returns bias metrics across gender
    and age groups.
    """
    model, le, prepare_features = _load_artifacts()

    csv_path = os.path.join(DATA_DIR, "synthetic_patients.csv")
    if not os.path.exists(csv_path):
        return {"error": "No dataset found. Run synthetic generator first."}

    df = pd.read_csv(csv_path)
    X, _ = prepare_features(df)
    y_true = le.transform(df["risk_level"])
    y_pred = model.predict(X)
    y_labels_true = le.inverse_transform(y_true)
    y_labels_pred = le.inverse_transform(y_pred)

    results: dict = {
        "overall_accuracy": round(float(accuracy_score(y_true, y_pred)), 4),
        "total_samples": len(df),
        "gender_analysis": _analyze_group(df, "gender", y_labels_true, y_labels_pred),
        "age_analysis": _analyze_age_groups(df, y_labels_true, y_labels_pred),
        "disparate_impact": _compute_disparate_impact(df, y_labels_pred),
        "demographic_parity": _compute_demographic_parity(df, y_labels_pred),
        "equalized_odds": _compute_equalized_odds(df, y_labels_true, y_labels_pred),
        "fairness_summary": "",
    }

    # Generate summary text
    results["fairness_summary"] = _generate_summary(results)

    return results


def _analyze_group(df: pd.DataFrame, group_col: str,
                   y_true: np.ndarray, y_pred: np.ndarray) -> list:
    """Per-group accuracy, precision, recall, and risk distribution."""
    groups = []
    for group_val in sorted(df[group_col].unique()):
        mask = df[group_col].values == group_val
        if mask.sum() == 0:
            continue

        gt = y_true[mask]
        pr = y_pred[mask]
        acc = float(accuracy_score(gt, pr))

        # Risk distribution in predictions
        risk_dist = {}
        for level in ["Low", "Medium", "High"]:
            risk_dist[level] = int((pr == level).sum())

        # Per-class metrics
        report = classification_report(gt, pr, output_dict=True, zero_division=0)
        per_class = {}
        for cls in ["Low", "Medium", "High"]:
            if cls in report:
                per_class[cls] = {
                    "precision": round(report[cls]["precision"], 4),
                    "recall": round(report[cls]["recall"], 4),
                    "f1": round(report[cls]["f1-score"], 4),
                }

        groups.append({
            "group": str(group_val),
            "count": int(mask.sum()),
            "accuracy": round(acc, 4),
            "risk_distribution": risk_dist,
            "per_class": per_class,
            "high_risk_rate": round(
                risk_dist.get("High", 0) / max(mask.sum(), 1), 4
            ),
        })

    return groups


def _analyze_age_groups(df: pd.DataFrame,
                        y_true: np.ndarray, y_pred: np.ndarray) -> list:
    """Analyze fairness across age brackets."""
    bins = [(0, 18, "0-18"), (19, 35, "19-35"), (36, 55, "36-55"),
            (56, 75, "56-75"), (76, 120, "76+")]
    groups = []
    for lo, hi, label in bins:
        mask = (df["age"].values >= lo) & (df["age"].values <= hi)
        if mask.sum() == 0:
            continue

        gt = y_true[mask]
        pr = y_pred[mask]
        acc = float(accuracy_score(gt, pr))

        risk_dist = {}
        for level in ["Low", "Medium", "High"]:
            risk_dist[level] = int((pr == level).sum())

        groups.append({
            "group": label,
            "count": int(mask.sum()),
            "accuracy": round(acc, 4),
            "risk_distribution": risk_dist,
            "high_risk_rate": round(
                risk_dist.get("High", 0) / max(mask.sum(), 1), 4
            ),
        })

    return groups


def _compute_disparate_impact(df: pd.DataFrame, y_pred: np.ndarray) -> dict:
    """
    Disparate Impact Ratio = P(High Risk | minority) / P(High Risk | majority).
    A ratio between 0.8 and 1.25 is generally considered fair (80% rule).
    """
    results = {}
    for group_col in ["gender"]:
        groups = sorted(df[group_col].unique())
        rates = {}
        for g in groups:
            mask = df[group_col].values == g
            n = mask.sum()
            if n == 0:
                continue
            high_count = (y_pred[mask] == "High").sum()
            rates[g] = round(float(high_count / n), 4)

        if len(rates) < 2:
            continue

        # Use the group with the highest rate as the reference
        max_rate = max(rates.values())
        if max_rate == 0:
            ratios = {g: 1.0 for g in rates}
        else:
            ratios = {g: round(r / max_rate, 4) for g, r in rates.items()}

        results[group_col] = {
            "rates": rates,
            "ratios": ratios,
            "fair": all(0.8 <= r <= 1.25 for r in ratios.values()),
        }

    return results


def _compute_demographic_parity(df: pd.DataFrame, y_pred: np.ndarray) -> dict:
    """
    Demographic Parity Difference: max difference in positive outcome rate
    across groups. Lower is better (ideal = 0).
    """
    results = {}
    for group_col in ["gender"]:
        groups = sorted(df[group_col].unique())
        high_rates = []
        group_rates = {}
        for g in groups:
            mask = df[group_col].values == g
            n = mask.sum()
            if n == 0:
                continue
            rate = float((y_pred[mask] == "High").sum() / n)
            high_rates.append(rate)
            group_rates[g] = round(rate, 4)

        if len(high_rates) < 2:
            continue

        diff = round(max(high_rates) - min(high_rates), 4)
        results[group_col] = {
            "group_rates": group_rates,
            "max_difference": diff,
            "fair": diff <= 0.10,  # < 10% difference is fair
        }

    return results


def _compute_equalized_odds(df: pd.DataFrame,
                            y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """
    Equalized Odds: compare True Positive Rates and False Positive Rates
    across groups for the 'High' class.
    """
    results = {}
    for group_col in ["gender"]:
        groups = sorted(df[group_col].unique())
        group_metrics = {}
        for g in groups:
            mask = df[group_col].values == g
            gt = y_true[mask]
            pr = y_pred[mask]

            # TPR for High
            actual_high = (gt == "High")
            if actual_high.sum() > 0:
                tpr = float((pr[actual_high] == "High").sum() / actual_high.sum())
            else:
                tpr = 0.0

            # FPR for High
            actual_not_high = (gt != "High")
            if actual_not_high.sum() > 0:
                fpr = float((pr[actual_not_high] == "High").sum() / actual_not_high.sum())
            else:
                fpr = 0.0

            group_metrics[g] = {
                "tpr": round(tpr, 4),
                "fpr": round(fpr, 4),
            }

        if len(group_metrics) < 2:
            continue

        tprs = [m["tpr"] for m in group_metrics.values()]
        fprs = [m["fpr"] for m in group_metrics.values()]
        tpr_diff = round(max(tprs) - min(tprs), 4)
        fpr_diff = round(max(fprs) - min(fprs), 4)

        results[group_col] = {
            "groups": group_metrics,
            "tpr_difference": tpr_diff,
            "fpr_difference": fpr_diff,
            "fair": tpr_diff <= 0.10 and fpr_diff <= 0.10,
        }

    return results


def _generate_summary(results: dict) -> str:
    """Generate a human-readable fairness summary."""
    issues = []
    passes = []

    di = results.get("disparate_impact", {})
    for col, data in di.items():
        if data.get("fair"):
            passes.append(f"Disparate Impact ({col}): PASS — all ratios within 80% rule")
        else:
            issues.append(f"Disparate Impact ({col}): FAIL — some groups fall outside 0.8-1.25 range")

    dp = results.get("demographic_parity", {})
    for col, data in dp.items():
        if data.get("fair"):
            passes.append(f"Demographic Parity ({col}): PASS — max difference {data['max_difference']:.1%}")
        else:
            issues.append(f"Demographic Parity ({col}): CONCERN — max difference {data['max_difference']:.1%}")

    eo = results.get("equalized_odds", {})
    for col, data in eo.items():
        if data.get("fair"):
            passes.append(f"Equalized Odds ({col}): PASS — TPR diff {data['tpr_difference']:.1%}, FPR diff {data['fpr_difference']:.1%}")
        else:
            issues.append(f"Equalized Odds ({col}): CONCERN — TPR diff {data['tpr_difference']:.1%}, FPR diff {data['fpr_difference']:.1%}")

    if issues:
        return "⚠ Fairness audit found potential concerns: " + "; ".join(issues) + ". " + " | ".join(passes)
    return "✅ Model passes all fairness checks. " + " | ".join(passes)
